---
title: "Modelo Obtención Log Odd-Ratio en Sujetos Control"
output: html_notebook
---

```{r}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(MASS)
library(fitdistrplus)
library(ggstatsplot)
library(tibble)
library(readr)
```

# Modelo Causal Tractos Esquizofrenia

Funciones

```{r}
logistic.regression.or.ci <- function(regress.out, level = 0.95) {
  usual.output <- summary(regress.out)
  z.quantile <- stats::qnorm(1 - (1 - level) / 2)
  number.vars <- length(regress.out$coefficients)
  OR <- exp(regress.out$coefficients[-1])
  temp.store.result <- matrix(rep(NA, number.vars * 2), nrow = number.vars)
  for (i in 1:number.vars) {
    temp.store.result[i, ] <- summary(regress.out)$coefficients[i] +
      c(-1, 1) * z.quantile * summary(regress.out)$coefficients[i + number.vars]
  }
  intercept.ci <- temp.store.result[1, ]
  slopes.ci <- temp.store.result[-1, ]
  OR.ci <- exp(slopes.ci)
  output <- list(
    regression.table = usual.output, intercept.ci = intercept.ci,
    slopes.ci = slopes.ci, OR = OR, OR.ci = OR.ci
  )
  return(output)
}
```

## Etapa 1 - definición de los datos expuestos y no expuestos

Obtener datos de sujetos de control y estudio. Ajustar para su análisis posterior

```{r}


# confusoras DAG= "sl2l_qa", "sl2l_iso","ecpl_qa", "ecpl_iso","age", "gender_F", 
# features predictoras = "sl2l_diameter","ccbd_diameter","ifol_diameter","ecpl_volume","tral_volume","sl2l_mean_length","ccbd_mean_length"

setwd("C:/hcgalvan/Repositorios/hcgalvan_project/data/union/End")
temp = gsub(".*target.*", "", readLines("integrado.csv"))
data<-read.table(text=temp, sep=",", header=TRUE)

t<-data.frame(data[,c("sl2l_diameter","ccbd_diameter","ifol_diameter","ecpl_volume","tral_volume","sl2l_mean_length","ccbd_mean_length","label","sl2l_qa", "sl2l_iso", "ccbd_iso", "ccbd_qa", "ifol_qa", "ifol_iso", "ecpl_qa", "ecpl_iso", "tral_qa", "tral_iso", "afsl_qa", "afsl_iso", "afsr_qa", "afsr_iso", "cfpl_iso", "cfpl_qa", "cfpr_iso", "cfpr_qa", "fatl_iso", "fatl_qa", "fatr_iso", "fatr_qa", "slfl_iso", "slfl_qa", "slfr_iso", "slfr_qa", "tral_iso", "tral_qa", "ufsl_iso", "ufsl_qa", "ufsr_iso", "ufsr_qa", "age", "gender_F")])

t$gend_F <- ifelse(t$gender_F == "True", 1, 0) #convierto en = o 1
t <- subset(t, select = -c(gender_F)) # Quito de tabla 
```

### Creación de features expuestos y no expuestos

Definición de nuevas variables como Expuesto y no Expuesto obtenido del analisis de su real distribución empírica. Donde False = No Expuesto = 0; True = Expuesto = 1

```{r}
 #  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 # 19.86   22.10   23.02   23.30   24.42   27.18
t<- t %>% 
  mutate(sl2ldmExp = if_else((t$sl2l_diameter > 22.10 & t$sl2l_diameter < 24.42), 0, 1))
#21.73   26.07   28.18   28.29   30.23   32.90 
t<- t %>% 
  mutate(ccbdmExp = if_else((t$ccbd_diameter > 26.07 & t$ccbd_diameter < 30.23), 0, 1))
#11.70   14.68   15.91   15.73   16.51   19.64
t<- t %>% 
  mutate(ifoldmExp = if_else((t$ifol_diameter > 14.68 & t$ifol_diameter < 16.51), 0, 1))
 #1785    6455    7407    7250    8918   10622 
t<- t %>% 
  mutate(ecplvlExp = if_else((t$ecpl_volume > 6455 & t$ecpl_volume < 8918), 0, 1))
#7780   12366   15988   15911   18004   26424 
t<- t %>% 
  mutate(tralvlExp = if_else((t$tral_volume > 12366 & t$tral_volume < 18004), 0, 1))
#65.44   72.18   75.69   76.94   80.61   88.53 
t<- t %>% 
  mutate(sl2lmlExp = if_else((t$sl2l_mean_length > 72.18 & t$sl2l_mean_length < 80.61), 0, 1))
  #99.87  112.62  117.22  117.47  123.81  131.73 
t<- t %>% 
  mutate(ccbdmlExp = if_else((t$ccbd_mean_length > 112.62 & t$ccbd_mean_length < 123.81), 0, 1))
```

```{r}
#par(mfrow = c(2, 2), mar = c(2, 2, 2, 2)) 
xtabs(~sl2ldmExp,t);xtabs(~ccbdmExp,t);xtabs(~ifoldmExp,t);xtabs(~ecplvlExp,t);xtabs(~tralvlExp,t);xtabs(~sl2lmlExp,t)
xtabs(~ccbdmlExp,t)
```

```{r}
tx<-c()
########################
tx$Esquiz <- ifelse(t$label == 0, "SI", "NO")
tx$sl2sdmd <- ifelse(t$sl2ldmExp == 0, "NO", "SI")
xtabs(~sl2sdmd+Esquiz,tx) #table of counts per stratum
odd <- ((31/27)/(15/23))
p <- (46/(23+15+27+31))
sprintf("odd: %f", odd)
sprintf("log odd: %f", log(odd))
sprintf("Prob(Ezq): %f", p)
v1 <- p*(27+31)
v2 <- (27+31)-v1
v3 <- p*(23+15)
v4 <- (23+15)-v3
pval <- 1-((v1/v2)/(v3/v3))
pval
((11.2/128.8)/(198.7/17.3))
########################
tx$ccbdmd <- ifelse(t$ccbdmExp == 0, "NO", "SI")
xtabs(~ccbdmd+Esquiz,tx) #table of counts per stratum
odd1 <- ((24/25)/(22/25)) ### ERROR
p1 <- (46/(50+46))
sprintf("odd: %f", odd1)
sprintf("log odd: %f", log(odd1))
sprintf("Prob(Ezq): %f", p1)

########################
tx$ifoldmd <- ifelse(t$ifoldmExp == 0, "NO", "SI")
xtabs(~ifoldmd+Esquiz,tx) #table of counts per stratum
odd2 <- ((27/30)/(16/23)) ### ERROR
p2 <- ((27+16)/(30+23+27+16))
sprintf("odd: %f", odd2)
sprintf("log odd: %f", log(odd2))
sprintf("Prob(Ezq): %f", p2)

########################
tx$sl2lmld <- ifelse(t$sl2lmlExp == 0, "NO", "SI")
xtabs(~sl2lmld+Esquiz,tx) #table of counts per stratum
odd3 <- ((24/25)/(22/25)) ### ERROR
p3 <- ((24+22)/(25+25+24+22))
sprintf("odd: %f", odd3)
sprintf("log odd: %f", log(odd3))
sprintf("Prob(Ezq): %f", p3)
########################
tx$ecplvld <- ifelse(t$ecplvlExp == 0, "NO", "SI")
xtabs(~ecplvld+Esquiz,tx) #table of counts per stratum
odd4 <- ((30/24)/(16/26)) ### ERROR
p4 <- ((30+16)/(24+26+30+16))
sprintf("odd: %f", odd4)
sprintf("log odd: %f", log(odd4))
sprintf("Prob(Ezq): %f", p4)

########################
tx$tralvld <- ifelse(t$tralvlExp == 0, "NO", "SI")
xtabs(~tralvld+Esquiz,tx) #table of counts per stratum
odd5 <- ((27/26)/(19/24)) ### ERROR
p5 <- ((27+19)/(26+24+27+19))
sprintf("odd: %f", odd5)
sprintf("log odd: %f", log(odd5))
sprintf("Prob(Ezq): %f", p5)

########################
tx$ccbdmld <- ifelse(t$ccbdmlExp == 0, "NO", "SI")
xtabs(~ccbdmld+Esquiz,tx) #table of counts per stratum
odd6 <- ((33/26)/(13/24)) ### ERROR
p6 <- ((33+13)/(26+24+33+13))
sprintf("odd: %f", odd6)
sprintf("log odd: %f", log(odd6))
sprintf("Prob(Ezq): %f", p6)
```

### Creación de variable A a partir de los expuestos y no expuestos

```{r}
#t <- subset(t, select = -c(A)) # Quito de tabla 
suma = 0
resultado <- c()
for (i in 1:length(t$label)){
#  suma = sum(t$sl2ldmExp[i], t$sl2lmlExp[i], t$ccbdmExp[i], t$ccbdmlExp[i], t$tralvlExp[i], t$ecplvlExp[i], t$ifoldmExp[i] )
#  suma = sum(t$sl2ldmExp[i], t$ccbdmlExp[i], t$tralvlExp[i], t$ecplvlExp[i], t$ifoldmExp[i] )
  suma = sum(t$ccbdmlExp[i], t$ecplvlExp[i]) # solo los 2 mas altos odds: 2.03; 2.34
#  suma = sum(t$sl2lmlExp[i], t$ccbdmExp[i])
#  suma = sum(t$sl2ldmExp[i], t$ccbdmlExp[i], t$ecplvlExp[i]) # solo dejo estos que tienen odds altos 1.76; 2.03; 2.34
  
  resultado[length(resultado) + 1] <- if_else((suma >=1 ),1, 0)
}
xtabs(~resultado)
t$A <- resultado
xtabs(~A,t)
sujeto_Ctrl <- dplyr::select(dplyr::filter(t, label == 1), "A")
sujeto_Est <- dplyr::select(dplyr::filter(t, label == 0), "A")
xtabs(~A,sujeto_Ctrl)
xtabs(~A,sujeto_Est)
```

## Etapa 2: Regresion Logística con PCA en Sujetos de Estudio - Expuestos.

#### Valor obtenido a partir del feature dependiente A y Confusoras como predictoras.

##### Reducir con PCA las predictoras.

```{r}
### Seleccion de features filtrando sujetos de control
t3 <- dplyr::select(dplyr::filter(t, label == 1), "sl2l_qa", "sl2l_iso", "ccbd_iso", "ccbd_qa", "ifol_qa", "ifol_iso", "ecpl_qa", "ecpl_iso", "tral_qa", "tral_iso", "afsl_qa", "afsl_iso", "afsr_qa", "afsr_iso", "cfpl_iso", "cfpl_qa", "cfpr_iso", "cfpr_qa", "fatl_iso", "fatl_qa", "fatr_iso", "fatr_qa", "slfl_iso", "slfl_qa", "slfr_iso", "slfr_qa", "tral_iso", "tral_qa", "ufsl_iso", "ufsl_qa", "ufsr_iso", "ufsr_qa", "age", "gend_F", "A")

```

```{r}
### No incluye esta lista a feature A para poder realizar escalado de datos
lista<-c("sl2l_qa", "sl2l_iso", "ccbd_iso", "ccbd_qa", "ifol_qa", "ifol_iso", "ecpl_qa", "ecpl_iso", "tral_qa", "tral_iso", "afsl_qa", "afsl_iso", "afsr_qa", "afsr_iso", "cfpl_iso", "cfpl_qa", "cfpr_iso", "cfpr_qa", "fatl_iso", "fatl_qa", "fatr_iso", "fatr_qa", "slfl_iso", "slfl_qa", "slfr_iso", "slfr_qa", "tral_iso", "tral_qa", "ufsl_iso", "ufsl_qa", "ufsr_iso", "ufsr_qa", "age", "gend_F")

```

#### Se realiza el normalizado para dejarlo en valores tipo Z

```{r}

pca.out3 <- prcomp(t3[,c(lista)],
                  scale. = TRUE)
#pca.out3$x
pca.var3 <- pca.out3$sdev^2  #autovalores
# Cálculo de la varianza explicada acumulada 
prop_varianza <- pca.out3$sdev^2/sum(pca.out3$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)
ggplot(data = data.frame(prop_varianza_acum, pc = factor(1:34)),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componentes principales", 
       y = "Prop. varianza explicada acumulada")
```

### Regresion logística binomial con PCA para obtener coeficientes

```{r}
#disable scientific notation for model summary
options(scipen=999)

#pca.var3
misdatos2 <- data.frame(A = t3[, "A"], pca.out3$x[, 1:2])

#pca.out3[1]
logRegPCANoExp <- glm(A ~ .,  data = misdatos2, family = binomial)

#summary(logRegPCA)
logistic.regression.or.ci(logRegPCANoExp)
summary(logRegPCANoExp)$coefficients
pscl::pR2(logRegPCANoExp)["McFadden"]
caret::varImp(logRegPCANoExp)
car::vif(logRegPCANoExp)

```

```{r}
library(robustbase)
library(survival)
str(t3)
datglmrob<-glmrob(formula = A ~ ., data= misdatos2, family = "binomial", method = "BY")
summary(datglmrob)

# pscl::pR2(datglmrob)["McFadden"]
```

```{r}
datglmrob$coefficients
```

```{r}
logRegPCANoExp$coefficients
```

#### Generación Propensity Z Score con PCA

Aquí es un trabajo manual para cada prueba a realizar. Cada regresión tiene que ser ajustada.

```{r}

############## PARA SUJETOS DE CONTROL #######################
zscores2<-c()
for(i in 1:50) {       # for-loop over rows
    zscores2[i] <-((1/(1+exp(-(as.numeric(unlist(logRegPCANoExp$coefficients[1]))+as.numeric(unlist(logRegPCANoExp$coefficients[2]))*as.numeric(unlist(pca.out3$x[i,"PC1"]))+as.numeric(unlist(logRegPCANoExp$coefficients[3]))*as.numeric(unlist(pca.out3$x[i,"PC2"])))))))
}
t3$zscore <-zscores2
quantile(t3$zscore)
```

### Piece-wice constant function - Approximate λ(Zi) using histogram splines

```{r}
# Utilizo los cuantiles de EXPUESTOS para armar stratos de zscore en LOS NO EXPUESTOS
#quintil <-quantile(t3$zscore, prob = seq(0, 1, 1/5))
#t3$subclass <- cut(x=t3$zscore,
#                              breaks=quantile(quintil, 
#                              prob = seq(0, 1, 1/5)),include.lowest=T)
cuantil <-quantile(t3$zscore, prob = seq(0, 1, 1/5))
t3$subclass <- cut(x=t3$zscore,
                              breaks=quantile(cuantil, 
                              prob = seq(0, 1, 1/5)),include.lowest=T)
levels(t3$subclass) <- 1:length(levels(t3$subclass))

t3$subclass
xtabs(~A+subclass,t3)
```

```{r}
# utilizar los quintiles y definir los valores medios
b1 = mean(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 1,),"zscore"))))
b2 = mean(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 2,),"zscore"))))
b3 = mean(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 3,),"zscore"))))
b4 = mean(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 4,),"zscore"))))
b5 = mean(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 5,),"zscore"))))

# Determinar los valores representativos
α_1 = exp(-b1)
α_2 = exp(-b2)
α_3 = exp(-b3)
α_4 = exp(-b4)
α_5 = exp(-b5)

### Obtengo los límites de zscore a partir del filtro subclase y utilizar en la función por partes

lim1 <- max(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 1,),"zscore"))))
lim2 <- max(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 2,),"zscore"))))
lim3 <- max(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 3,),"zscore"))))
lim4 <- max(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 4,),"zscore"))))
lim5 <- max(as.numeric(unlist(dplyr::select(dplyr::filter(t3, subclass == 5,),"zscore"))))


fx <- function(x, dat){
    f <- NULL
    ifelse (x > 0 & x<= lim1, f <- α_1, 0)
    ifelse (x>lim1 & x<=lim2, f <- α_2, 0)
    ifelse (x>lim2 & x<=lim3, f <- α_3, 0)
    ifelse (x>lim3 & x<=lim4, f <- α_4, 0)
    ifelse (x>lim4, f <- α_5, 0)
    f
}

x <- data.frame(t3$zscore)
for(i in 1:nrow(x)) {       # for-loop over rows
  lambd <- fx(t3$zscore[[i]])
  t3$lambda[i] <- lambd
}

plot(t3$lambda, type="l", las=1)
points(x)

```

### Ajustar con MLE (Maximum Likelihood Estimation) λ(Zi)

Buscamos ajustar los coeficientes obtenidos en la función Lambda (función constante por partes a partir de histogram splines) que son los α_1...α_k

```{r}
library(stats4) # para la función mle
library(bbmle) # para la función mle2


# cuales son los pasos que necesitamos hacer para ajustar como problema MLE?
# 1 Pull out the design matrix for the model
# 2 - write support function/ NLL calculator
# 3 decide on some starting values for numerical optimization
# 4 - fit the model !! (Decide on our "black box" for optimitation method)
# 5 - check the model fit, and whether it converged..(profile)
# 6 - confirm the results do not depend on the starting values.
# If the model has not converged.. then what?
  # - change starting values
   # try different optimization method("SANN" or Nelder-Mead)

# Step 1 get the design matrix 
 #- armo nuevo data frame con armado de features a partir de las subclases


design_matrix <- model.matrix(~ subclass, data=t3)
head(design_matrix)

# Utilizo además https://biol607.github.io/2012/ Exercise 14 on if (sigma <= 0) return(NaN)
# 2 - write support function/ NLL calculator

# Forma 1
glm_MLE_support <- function(b0, b1, b2, b3, b4, b5, sigma, y = as.numeric(t3$subclass), X=design_matrix, dat =t3$zscore){
  deterministic_part <- b0+ as.numeric(ifelse((X[,2]+X[,3]+X[,4]+X[,5])==1,0,b1*X[,1]*dat) + b2*X[,2]*dat + b3*X[,3]*dat + b4*X[,4]*dat + b5*X[,5]*dat)
  if (sigma <= 0) return(NaN) 
  -sum(dnorm(y, mean = as.numeric(deterministic_part), sd=as.numeric(sigma), log=T))
}

# Forma 2
glm_MLE_support_1 <- function(b0, b1, b2, b3, b4, sigma, y = as.numeric(t3$subclass), X=design_matrix, dat =t3$zscore){
  deterministic_part <- as.numeric(ifelse((X[,2]+X[,3]+X[,4]+X[,5])==1,0,b0*X[,1]*dat) + b1*X[,2]*dat + b2*X[,3]*dat + b3*X[,4]*dat + b4*X[,5]*dat)
  if (sigma <= 0) return(NaN) 
  -sum(dnorm(y, mean = as.numeric(deterministic_part), sd=as.numeric(sigma), log=T))
}

# glm_MLE_support(1,1,1,1,0,1,0.7441488)
# 3 decide on some starting values for numerical optimization

# let's use the standard error of the mean for sigma
SE <- function(x){
  as.numeric(sd(as.numeric(x))/sqrt(as.numeric(length(x))))}

# dat <- as.factor
# 4 try a fit
# glm.MLE.fit <- mle2(glm_MLE_support,
#                    start=list(b0 = α_1 , b1=α_2, b2=α_3, b3=α_4, b4=α_5, sigma = SE(t3$zscore)), method = "SANN")

# Forma 1
glm.MLE.fit <- mle2(glm_MLE_support,
                    start=list(b0 =0 , b1=α_1 , b2=α_2, b3=α_3, b4=α_4, b5=α_5, sigma = SE(t3$zscore)))

# Forma2
glm.MLE.fit_1 <- mle2(glm_MLE_support_1,
                    start=list(b0=α_1 , b1=α_2, b2=α_3, b3=α_4, b4=α_5, sigma = SE(t3$zscore)))

# 
summary(glm.MLE.fit)
summary(glm.MLE.fit_1)


glm.MLE.Null <- mle2(glm_MLE_support,
                    start=list(b0 = α_1 , b1=α_2, b2=α_3, b3=α_4, b4=α_5, sigma = SE(t3$zscore)),
                    fixed=list(b0=0 , b1=0, b2=0, b3=0, b4=0))


confint(glm.MLE.Null)
confint(glm.MLE.fit)
anova(glm.MLE.fit, glm.MLE.Null)
# anova(str(glm.MLE.fit), test="Chisq") no funciona con este tipo de funcion nll
  
plot(profile(glm.MLE.fit))
plot(profile(glm.MLE.fit_1))
summary(glm.MLE.fit)
confint(glm.MLE.fit)
AIC(glm.MLE.fit)
AIC(glm.MLE.Null)
# Para hacer despues, ploteo correcto https://stackoverflow.com/questions/57153916/how-do-i-plot-a-mle2-fit-of-a-model-in-ggplot2-along-with-the-data
# la busqueda en google fue ggplot(profile(mle2))
# ggplot(profile(glm.MLE.fit_1))
#5 look at fit
#library(arulesViz) no es necesario
#library(Rgraphviz) no es necesario
# dismo::plot no es necesario
################################
# Probando los coeficientes
glm.MLE.fit_1 <- mle2(glm_MLE_support_1,
                    start=list(b0=1.3779501  , b1=2.6703470  , b2=3.9454824  , b3=5.1855838  , b4=6.2961919  , sigma = 0.0320563  ))
summary(glm.MLE.fit_1)
plot(profile(glm.MLE.fit_1))
```

### Armar la estructura de datos en D, A, λ(Zi)

```{r}
# https://stats.stackexchange.com/questions/568322/students-t-distribution-and-ml-in-
# https://rpubs.com/ramblinguy/meh

t3$D <- 0 #valor de sujetos de control

#The function below calculates gradient.
#Shamelessly taken from the site mentioned above

sigmoid = function(z){
  1/(1+exp(-z))
}

cost_logistic = function(X, y, theta){
  h_theta = sigmoid(X%*%theta)
  cost = 1/nrow(X)*sum(-y*log(h_theta)-(1-y)*log(1-h_theta))
  return(cost)
}

my_gradient=function(theta, X, y){
    
    h_theta= sigmoid(X%*%theta)
    
    ## OPTION 1-- looping
    
    #    gradient=rep(0,ncol(X))
    #      for(j in 1:ncol(X)){
    #       for(i in 1:nrow(X)){
    #    gradient[j]=gradient[j]+1/nrow(X)*(my_sigmoid(X[i,]%*%theta)-y[i])*X[i,j]
    #                   }
    #               }
    
    # option2-more succint
   gradient= 1/nrow(X)*(sigmoid(t(theta)%*%t(X))-t(y))%*%X
    return(gradient)                                                                           
}
X = model.matrix(~subclass, data=t3) # Armo matriz de dummies para coeficientes

dat <- t3$zscore  # valores de zscore a utilizar en la función lambda
b0 <- 1.3779501 # coef 1 - Intercept
b1 <- 2.6703470 # coef 2
b2 <- 3.9454824 # coef 3
b3 <- 5.1855838 # coef 4
b4 <- 6.2961919 # coef 5
###

logistic_X = cbind(t3$A,(as.numeric(ifelse((X[,2]+X[,3]+X[,4]+X[,5])==1,0,b0*X[,1]*dat) + b1*X[,2]*dat + b2*X[,3]*dat + b3*X[,4]*dat + b4*X[,5]*dat))) # Matriz

initial_theta =matrix(rep(0,ncol(logistic_X)))
my_gradient(initial_theta,logistic_X,t3$D)


reg_opt_BFGS =optim(par = initial_theta, X=logistic_X, y=t3$D,fn=cost_logistic, gr = my_gradient, method = "BFGS", hessian = TRUE)
reg_opt_NM = optim(par = initial_theta,X=logistic_X,y=t3$D,fn=cost_logistic, method = "Nelder-Mead", hessian = TRUE)
reg_opt_BFGS
reg_opt_NM
mle <-optim(par = initial_theta,X=logistic_X, y=t3$D, fn=cost_logistic, method = "BFGS", hessian = TRUE)



reg_parc = glm(D~A+(as.numeric(ifelse((X[,2]+X[,3]+X[,4]+X[,5])==1,0,b0*X[,1]*dat) + b1*X[,2]*dat + b2*X[,3]*dat + b3*X[,4]*dat + b4*X[,5]*dat)), data=t3, family = "binomial")

summary(reg_parc)

```

```{r}
# https://arunaddagatla.medium.com/maximum-likelihood-estimation-in-logistic-regression-f86ff1627b67
# 1. Prefix -------------------------------------------------------------------
# Remove all files from ls
# rm(list = ls())

# Loading packages
require(aplore3)
require(optimx)
require(numDeriv)
require(dplyr)

# 2. Loading dataset ----------------------------------------------------------
X = model.matrix(~subclass, data=t3) # Armo matriz de dummies para coeficientes

dat <- t3$zscore  # valores de zscore a utilizar en la función lambda
b0 <- 1.3779501 # coef 1 - Intercept
b1 <- 2.6703470 # coef 2
b2 <- 3.9454824 # coef 3
b3 <- 5.1855838 # coef 4
b4 <- 6.2961919 # coef 5

λzi <- (as.numeric(ifelse((X[,2]+X[,3]+X[,4]+X[,5])==1,0,b0*X[,1]*dat) + b1*X[,2]*dat + b2*X[,3]*dat + b3*X[,4]*dat + b4*X[,5]*dat))

# 3. Define log-likelihood function for logistic regression model -------------
# (see applied logistic regression)
negll <- function(par){
  
  #Extract guesses for alpha and beta1
  alpha <- par[1]
  beta1 <- par[2]
  beta2 <- par[3]
  
  #Define dependent and independent variables
  y  <- t3$D
  x1 <- t3$A
  x2 <- λzi
  
  #Calculate pi and xb
  xb <- alpha + beta1 * x1 + beta2 * x2
  pi <- exp(xb) / (1 + exp(xb))
  
  #Set high values for 0 < pi < 1
  if(any(pi > 1) | any(pi < 0)) {
    val <- 1e+200
  } else {
    val <- -sum(y * log(pi) + (1 - y) * log(1 - pi))
  }
  val
}
# 4. Define fradient function for logistic regression model -------------------
# (see applied logistic regression)
negll.grad <- function(par){
  
  #Extract guesses for alpha and beta1
  alpha <- par[1]
  beta1 <- par[2]
  beta2 <- par[3]
  
  #Define dependent and independent variables
  y  <- t3$D
  x1 <- t3$A
  x2 <- λzi
  
  #Create output vector
  n <- length(par[1])
  gg <- as.vector(rep(0, n))
  
  #Calculate pi and xb
  xb <- alpha + beta1 * x1 + beta2 * x2
  pi <- exp(xb) / (1 + exp(xb))
  
  #Calculate gradients for alpha and beta1
  gg[1] <- -sum(y - pi)
  gg[2] <- -sum(x1 * (y - pi))
  gg[3] <- -sum(x2 * (y - pi))
  
  return(gg)
}

# 4.1 Compare gradient function with numeric approximation of gradient ========
# compare gradient at 0, 0, 0
mygrad <- negll.grad(c(0, 0, 0))
numgrad <- grad(x = c(0, 0, 0), func = negll)

all.equal(mygrad, numgrad)

# 4. Find maximum of log-likelihood function ----------------------------------
opt <- optimx(par = c(alpha  = 0,
                      beta_1 = 0, 
                      beta_2 = 0), 
              fn = negll, 
              gr = negll.grad, 
              control = list(trace = 0, 
                             all.methods = TRUE))

# print reulsts of optimisation
# remove not needed information for purpose of presentation 
summary(opt, order = "convcode") %>% 
  dplyr::select(-value, -fevals, -gevals ,-niter )

#value fevals gevals niter
# 5. Estimate regression coeficents using glm ---------------------------------
glm_model <- glm(D ~ A + λzi, 
                 data = t3,
                 family = binomial(link = "logit"))

# Print coefficents
coef(glm_model)


# 6. Comparing results from optimx and glm ------------------------------------
glm_results <- unname(coef(glm_model))
coef_opt <- coef(opt)

lapply(1:nrow(coef_opt), function(i){
    
    optimisation_algorithm <- attributes(coef_opt)$dimnames[[1]][i]

    mle_glm1 <- (coef_opt[i, "alpha" ] - glm_results[1])
    mle_glm2 <- (coef_opt[i, "beta_1"] - glm_results[2])
    mle_glm3 <- (coef_opt[i, "beta_2"] - glm_results[3])
    
    mean_difference <- mean(mle_glm1, mle_glm2, mle_glm3, na.rm = TRUE)
    
    data.frame(optimisation_algorithm, mean_difference)
    
  }) %>% 
    bind_rows() %>% 
  filter(!is.na(mean_difference)) %>% 
  mutate(mean_difference = abs(mean_difference)) %>% 
  arrange(mean_difference)


```

```{r}
# https://www.joshua-entrop.com/post/optim_logit_reg/
  
# 1. Prefix -------------------------------------------------------------------

# Remove all files from ls
rm(list = ls())

# Loading packages
require(aplore3)
require(optimx)
require(numDeriv)
require(dplyr)

# 2. Loading dataset ----------------------------------------------------------

#Reading the example data set icu from the package aplore3
icu <- as.data.frame(icu)
icu$sta_n <- ifelse(icu$sta == "Died", 1, 0)
icu$female <- ifelse(icu$gender == "Female", 1, 0)


# 3. Define log-likelihood function for logistic regression model -------------
# (see applied logistic regression)
negll <- function(par){
  
  #Extract guesses for alpha and beta1
  alpha <- par[1]
  beta1 <- par[2]
  beta2 <- par[3]
  
  #Define dependent and independent variables
  y  <- icu$sta_n
  x1 <- icu$age
  x2 <- icu$female
  
  #Calculate pi and xb
  xb <- alpha + beta1 * x1 + beta2 * x2
  pi <- exp(xb) / (1 + exp(xb))
  
  #Set high values for 0 < pi < 1
  if(any(pi > 1) | any(pi < 0)) {
    val <- 1e+200
  } else {
    val <- -sum(y * log(pi) + (1 - y) * log(1 - pi))
  }
  val
}


# 4. Define fradient function for logistic regression model -------------------
# (see applied logistic regression)
negll.grad <- function(par){
  
  #Extract guesses for alpha and beta1
  alpha <- par[1]
  beta1 <- par[2]
  beta2 <- par[3]
  
  #Define dependent and independent variables
  y  <- icu$sta_n
  x1 <- icu$age
  x2 <- icu$female
  
  #Create output vector
  n <- length(par[1])
  gg <- as.vector(rep(0, n))
  
  #Calculate pi and xb
  xb <- alpha + beta1 * x1 + beta2 * x2
  pi <- exp(xb) / (1 + exp(xb))
  
  #Calculate gradients for alpha and beta1
  gg[1] <- -sum(y - pi)
  gg[2] <- -sum(x1 * (y - pi))
  gg[3] <- -sum(x2 * (y - pi))
  
  return(gg)
}

# 4.1 Compare gradient function with numeric approximation of gradient ========
# compare gradient at 0, 0, 0
mygrad <- negll.grad(c(0, 0, 0))
numgrad <- grad(x = c(0, 0, 0), func = negll)

all.equal(mygrad, numgrad)

# 4. Find maximum of log-likelihood function ----------------------------------
opt <- optimx(par = c(alpha  = 0,
                      beta_1 = 0, 
                      beta_2 = 0), 
              fn = negll, 
              gr = negll.grad, 
              control = list(trace = 0, 
                             all.methods = TRUE))

# print reulsts of optimisation
# remove not needed information for purpose of presentation 
summary(opt, order = "convcode") %>% select(-value, -niter, -gevals, -fevals)

# 5. Estimate regression coeficents using glm ---------------------------------
glm_model <- glm(sta_n ~ age + female, 
                 data = icu,
                 family = binomial(link = "logit"))

# Print coefficents
coef(glm_model)


# 6. Comparing results from optimx and glm ------------------------------------
glm_results <- unname(coef(glm_model))
coef_opt <- coef(opt)

lapply(1:nrow(coef_opt), function(i){
    
    optimisation_algorithm <- attributes(coef_opt)$dimnames[[1]][i]

    mle_glm1 <- (coef_opt[i, "alpha" ] - glm_results[1])
    mle_glm2 <- (coef_opt[i, "beta_1"] - glm_results[2])
    mle_glm3 <- (coef_opt[i, "beta_2"] - glm_results[3])
    
    mean_difference <- mean(mle_glm1, mle_glm2, mle_glm3, na.rm = TRUE)
    
    data.frame(optimisation_algorithm, mean_difference)
    
  }) %>% 
    bind_rows() %>% 
  filter(!is.na(mean_difference)) %>% 
  mutate(mean_difference = abs(mean_difference)) %>% 
  arrange(mean_difference)


```

### Forma estratificada de regresión \# A modo de haber realizado el test

Esta forma no es viable con la regresion parcial, requiere de al menos 2 valores intercalados (0 y 1)

library(survival) , reglog.cond \<-clogit(D\~A+strata(lambda),data=t3)

### Forma de hallar Coeficientes de Regresion Logistica Parcial D=1\~A+λ(Zi)

```{r}
library(caret)
#https://rpubs.com/ramblinguy/meh

t3$D <- 0 #valor de sujetos de control

#The function below calculates gradient.
#Shamelessly taken from the site mentioned above

sigmoid = function(z){
  1/(1+exp(-z))
}

cost_logistic = function(X, y, theta){
  h_theta = sigmoid(X%*%theta)
  cost = 1/nrow(X)*sum(-y*log(h_theta)-(1-y)*log(1-h_theta))
  return(cost)
}

my_gradient=function(theta, X, y){
    
    h_theta= sigmoid(X%*%theta)
    
    ## OPTION 1-- looping
    
    #    gradient=rep(0,ncol(X))
    #      for(j in 1:ncol(X)){
    #       for(i in 1:nrow(X)){
    #    gradient[j]=gradient[j]+1/nrow(X)*(my_sigmoid(X[i,]%*%theta)-y[i])*X[i,j]
    #                   }
    #               }
    
    # option2-more succint
   gradient= 1/nrow(X)*(sigmoid(t(theta)%*%t(X))-t(y))%*%X
    return(gradient)                                                                           
}
logistic_X = cbind(1,t3$A,t3$lambda)
logistic_X = cbind(1,t3$A)
initial_theta =matrix(rep(0,ncol(logistic_X)))
my_gradient(initial_theta,logistic_X,t3$D)

```

```{r}
#plot of sigmoid
sig_x = t3$zscore
sig_y = sigmoid(sig_x)
plot(sig_x,sig_y,type = "l")

y = sig_x + rnorm(50,0,2)+1
plot(x,y)
```

```{r}
#We can see the inital values for the gradient at our inital theta

optim(par = initial_theta,X=logistic_X,y=t3$D,fn=cost_logistic, gr = my_gradient)
```

```{r}
optim(par = initial_theta,X=logistic_X,y=t3$D,fn=cost_logistic, method = "Nelder-Mead")
```

```{r}
optim(par = initial_theta,X=logistic_X,y=t3$D,fn=cost_logistic, method = "BFGS")

```

```{r}
mle <-optim(par = initial_theta,X=logistic_X,y=t3$D,fn=cost_logistic, method = "BFGS", hessian = TRUE)
mle
```

```{r}
plot(y ~ x, data = t3, main="Least square regression")
abline(a = mle$par[1], b = mle$par[2], col = "red")
```

```{r}

```

### Forma Regresion Logistica Lineal para encontrar coeficientes

```{r}
t3$D <- 0
form = "D ~ A+lambda"
form = formula(form)
model1_ezq = glm(form,data=t3,family=binomial(link="logit"))

# solo funciona con el modelo realizado con glm
pscl::pR2(model1_ezq)["McFadden"]
caret::varImp(model1_ezq)

summary(model1_ezq)
coef(model1_ezq)
confint(model1_ezq)
logLik(model1_ezq)

head(model.matrix(model1_ezq))
tail(model.matrix(model1_ezq))
###############################

design_mat <- with(data=t3, model.matrix(~A+lambda))
head(design_mat)
tail(design_mat)

design_mat[,3]
###############
# negative log likelihood calculator (objective function)
glm.mle = function(b0,b1,b2,sigma, x=design_mat){
  Y.pred = b0+b1*x[,2]+b2*x[,3]
  -sum(dnorm(t3$D, mean=Y.pred, sd=sigma, log=TRUE))
}

glm.mle(1,1,0,1,design_mat)
###############
start.b0 <- mean(t3$D)
start.sigma <- sd(t3$D)

mle.model <- mle2(glm.mle, start=list(b0=start.b0, b1=0, b2=0, sigma=start.sigma))
summary(mle.model)

########## 
par(mfrow=c(1,3))
prof <- profile(mle.model)
plot(prof, conf=c(99,95,90,80,50)/100, absVal=T)

confint(prof)
vcov(mle.model)
logLik(mle.model)

### Using this for a likelihood ratio test
### How do we calculate the reduced model? IN this case it easy

glm.mle.reduced = function(b0, sigma=0.1){
  Y.pred = b0
  -sum(dnorm(t3$D, mean = Y.pred, sd = sigma, log = TRUE))
}

mle.model.reduced <- mle(glm.mle.reduced,
                         list(b0=start.b0, sigma=start.sigma),
                         method="BFGS")
summary(mle.model.reduced)

Likelihood_ratio = -2*(logLik(mle.model.reduced)- logLik(mle.model))
pchisq(Likelihood_ratio[1], df=1, lower.tail = F)

# can use this approach with the anova function using test=Chisq
anova(model1_ezq, test="Chisq")

#or whith a null
Model1_null = lm(D ~ 1, data=t3)
anova(Model1_null, model1_ezq, test="Chisq")

# It may be far more appropriate to use information criteria
AIC(mle.model.reduced)
AIC(mle.model)

# in the bbmle library 
ICtab(mle.model, mle.model.reduced,
      type = "AICc",
      weights = T, #Akaike weights
      base = T,    # values for ICs (not just deltas)
      nobs= nrow(t3)) # numero de observaciones, necesarios para AICc


```

```{r}
library(tidyverse)

td <- t3 |> mutate(across(c(A:lambda)))  
td
```

```{r}
# str(t3)

form = "A~D+lambda"
form = formula(form)
#reg = glm(form,data=t3,family=binomial(link="logit"))
reg = glm(form,data=t3,family=binomial)
summary(reg)
logistic.regression.or.ci(reg)
deviance(reg)/df.residual(reg)

```

### Forma - Generar Maxima verosimilitud de funcion aproximada

Para ajuste de modelos

<https://www.youtube.com/watch?v=wPtHpEp-jRo>

```{r}


library(stats4) # para la función mle
library(bbmle) # para la función mle2
#########
#NegLogLik = function(mu,sigma){-sum(dnorm(as.numeric(unlist(x)),as.numeric(mu),as.numeric(unlist(sigma),log = TRUE)))}
#EMV1 = mle(NegLogLik, start = list(mu=10, sigma=5))
#summary(EMV1)
#confint(EMV1)
#########

mu <- mean(t3$lambda)
sigma <- sqrt(sum((t3$lambda-mean(t3$lambda))^2)/(length(t3$lambda)-1))
x <- t3$lambda
sigma
EMV1 = mle2(x~dnorm(mu,sigma),start = list(mu=0,sigma=1), data=data.frame(x))
summary(EMV1)
confint(EMV1)
coef(EMV1)
vcov(EMV1)

# https://stackoverflow.com/questions/32567582/asymptotic-variance-of-maximum-likelihood-estimator-with-optim-in-r

# https://rstudio-pubs-static.s3.amazonaws.com/415830_829dd2b0150d438894c0e6695e7e6721.html


```

No se puede realizar regresion penalizada porque no tengo 2 valores en y que es D, ya que es parcial

pero tambien tengo problemas porqque la max verosimilitud es sesgada con pocos datos.

<https://statisticalhorizons.com/logistic-regression-for-rare-events/>

<http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/>

<https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/>

<https://stackoverflow.com/questions/24811598/error-in-dropy-rep1-nc-error-for-cv-glmnet-in-glmnet-r-package>

```{r}
library(glmnet)
t3$subclass
data<- data.frame(t3[,c('D','A','subclass')])
x <- model.matrix(D~., data)[,-1]
y <- as.matrix(sample(1,50,replace=TRUE))
str(y)
str(x)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
is.na(y)

glmnet(x, y, family = "binomial", alpha = 1, lambda = NULL)

hist(t3$zscore, probability = T ,main = "Histogram of Count Variable")
lines(density(t3$A), col="red", lwd=2)
plotdist(t3$zscore, histo=TRUE, demp=TRUE)
descdist(t3$lambda, boot = 1000)
```

```{r}
lymax <- c(0,1)
lhalf <- 0
x <- sort(t3$D)
g <- t3$A
y <- t3$lambda
d2 <- data.frame(x,g,y)

fit3 <- mle2(y~dnbinom(mu=exp(lymax)/(1+x/exp(lhalf)),size=exp(logk)),
    parameters=list(lymax~g),data=d2,
    start=list(lymax=0,lhalf=0,logk=0))

summary(fit3)

```

```{r}
plot(t3$zscore)
```

#### Generar Constante Delta

Se utiliza o genera despues de realizar la regresión

```{r}
### Regresion Logística de A en lambda
form = "A ~ lambda"
form = formula(form)
reg = glm(form,data=t3,family=binomial)
summary(reg)

### Se logra con la reg logistica entre A y lambda(z)
δ<-deviance(reg)/df.residual(reg) # ratio varianza para Delta
delta <-deviance(reg)/df.residual(reg)
```

## Modelo Final

El modelo final tiene algunas dificultades, con respecto a si es por cada sujeto o es el total de sujetos.

Deberíamos tener dos calculos

```{r}
##################################################################
#función productoria
productoria <- function(D,delta,A,lamdaz){
   result = prod((exp(D*(delta*A)+lamdaz))/(1+exp((delta*A)+lamdaz)))
   return(result)
}
# productoria(1,.9,1,2)
## donde D es label donde es 1 = Patologia, 0 sin patología, 
log_Ratio_odds = exp(t3$D*(delta*t3$A)+t3$lambda) / (1+(exp((delta*t3$A)+t3$lambda)))
plot(log_Ratio_odds)

log_ODDs_Ratios <- productoria(t3$D, delta, t3$A, t3$lambda)
multiplica = 1
for (i in 1:length(t3$D)){
  multiplica = multiplica*productoria(t3$D[i], delta, t3$A[i], t3$lambda[i])
}
multiplica
log_ODDs_Ratios
```

#### Obtener la varianza asintotica

```{r}
library(optimx)

# Define the objective function
obj_fn <- function(par, x, y) {
  a <- par[1]
  b <- par[2]
  sum((y - a * exp(-b * x))^2)
}

# Generate some example data
set.seed(123)
x <- runif(100, 0, 10)
y <- 2 * exp(-0.3 * x) + rnorm(100, 0, 0.1)

genrose.h <- function(x, gs=NULL) { ## compute Hessian
   if(is.null(gs)) { gs=100.0 }
	n <- length(x)
	hh<-matrix(rep(0, n*n),n,n)
	for (i in 2:n) {
		z1<-x[i]-x[i-1]*x[i-1]
		z2<-1.0-x[i]
                hh[i,i]<-hh[i,i]+2.0*(gs+1.0)
                hh[i-1,i-1]<-hh[i-1,i-1]-4.0*gs*z1-4.0*gs*x[i-1]*(-2.0*x[i-1])
                hh[i,i-1]<-hh[i,i-1]-4.0*gs*x[i-1]
                hh[i-1,i]<-hh[i-1,i]-4.0*gs*x[i-1]
	}
        return(hh)
}

# Fit the model
fit <- optimx(par = c(1, 1), fn = obj_fn, x = x, y = y, hess=genrose.h)

# Extract the Hessian matrix
hessian <- fit$hessian
hessian
# Calculate the asymptotic variance-covariance matrix
n <- length(x)
avar_cov <- solve(hessian) / n

# Extract the asymptotic variances
asymptotic_variances <- diag(avar_cov)

# Print the results
print(asymptotic_variances)
```

```{r}
library(optimx)

# Define the objective function
obj_fn <- function(par, x, y) {
  a <- par[1]
  b <- par[2]
  sum((y - a * exp(-b * x))^2)
}

# Generate some example data
set.seed(123)
x <- runif(100, 0, 10)
y <- 2 * exp(-0.3 * x) + rnorm(100, 0, 0.1)

# Fit the model
fit <- optimx(par = c(1, 1), fn = obj_fn, x = x, y = y)

# Extract the Hessian matrix
hessian <- fit$hessian

# Calculate the asymptotic variance-covariance matrix
n <- length(x)
avar_cov <- solve(hessian) / n

# Extract the asymptotic variances
asymptotic_variances <- diag(avar_cov)

# Calculate the asymptotic variance ratio
param1_index <- which(names(asymptotic_variances) == "par1")
param2_index <- which(names(asymptotic_variances) == "par2")
avar_ratio <- asymptotic_variances[param1_index] / asymptotic_variances[param2_index]

# Print the resul
```

```{r}

```
