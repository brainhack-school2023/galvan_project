---
title: "Analitica ccbd_mean_length"
output: html_notebook
---

### Conclusiones del Análisis ccbd_mean_length

Buscamos en el feature ccbd_mean_length un estimador de posición y establecer su distribución.

Estos pasos siguientes fueron realizados a partir de exploración gráfica de los datos y bajo conocimiento de su captura y procesamiento.

**Paso 1**: separar valores en control y estudio.\
**Paso 2:** verificar valores repetidos.En este caso no existen repetidos.\
**Paso 3:** Limpiar los outliers, de manera general cuando existe cuando se juntan ambas particiones.

Los outliers son mínimos y afectan muy poco al valor final de los estimadores de posición, sin embargo en ajuste a distribución teórica, y al tener cola pesada izquierda ajusta parcialmente, utilizamos además ajustes con diferentes nucleos y ancho de banda.

Con prechequeo formal formal de normalidad (**shapiro-wilk y Anderson-Darling)**, sostiene la hipótesis de normalidad.

Por otro lado, es importante analizar las distribuciones separadas entre Control y Estudio en este features para verificar sus diferencias.

Entonces, nuevamente es importante ajustar la distribución lo mejor posible a su promedio, para lograr diferenciar y llegar a la verdadera posición central. (ver la necesidad del tamaño de muestra ante el uso del sistema libre distribucion)

Podemos pensar en una distribución teorica normal y pensar en Silverman para establecer la posición y sus desviaciones ajustadas.

```{r}
library(ggplot2)
library(gridExtra)
library(MASS)
library(fitdistrplus)
library(ggstatsplot)
library(BSSasymp) #Cramer-ca

```

```{r}
### Ubicacion de datos
setwd("C:/hcgalvan/Repositorios/hcgalvan_project/data/union/End")
temp = gsub(".*target.*", "", readLines("seleccionestudio.csv"))
data<-read.table(text=temp, sep=",", header=TRUE)
ccbdlm <-data$ccbd_mean_length

```

#### Tratamiento Ajustado a Distribución y comparativa con/sin outliers

Separamos los datos de la variable de estudio

```{r}
Control<-(as.numeric(unlist(subset(data, label==1, select=c(ccbd_mean_length)))))
Estudio<-(as.numeric(unlist(subset(data, label==0, select=c(ccbd_mean_length)))))
Controlclean<-unique(Control)
Estudioclean<-unique(Estudio)

```

```{r}
boxplot(Control, Estudio, ccbdlm)$out
```

```{r}
Q <- quantile(data$ccbd_mean_length, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(data$ccbd_mean_length)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
datasinout<- subset(data, data$ccbd_mean_length > (Q[1] - 1.5*iqr) & data$ccbd_mean_length < (Q[2]+1.5*iqr))
```

```{r}
boxplot(datasinout$ccbd_mean_length)$out
```

Graficamos las distribuciones obtenidos.

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) 
plotdist(datasinout$ccbd_mean_length, histo=TRUE, demp=TRUE) #completamente ajustado
plotdist(data$ccbd_mean_length, histo=TRUE, demp=TRUE) #completamente ajustado

```

Realizamos descripcion de los valores descriptivos: Ajuste total, parcial y sin ajuste

```{r}
summary(datasinout$ccbd_mean_length) 
summary(data$ccbd_mean_length)

```

Graficamos la densidad de distribución: Ajuste Total, Parcial y sin ajustes

```{r}
descdist(datasinout$ccbd_mean_length, boot = 1000)
descdist(data$ccbd_mean_length, boot = 1000)
```

Ajustamos los datos a la distribución Normal

```{r}
library(glogis) 
total<-fitdist(datasinout$ccbd_mean_length, "norm")
sinajuste<-fitdist(data$ccbd_mean_length, "norm") 

summary(total) 
summary(sinajuste) 

```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) 
fb <- fitdist(datasinout$ccbd_mean_length, "norm") 
plot.legend <- c("norm") 
denscomp(list(fb), legendtext = plot.legend) 
qqcomp(list(fb), legendtext = plot.legend) 
cdfcomp(list(fb), legendtext = plot.legend) 
ppcomp(list(fb), legendtext = plot.legend)
```

```{r}
library(actuar)  

ATV1 <- datasinout$ccbd_mean_length
datos1.l<-fitdist(ATV1, "norm")
cdfcomp(list(datos1.l), xlogscale = TRUE, 
        ylogscale = TRUE, legendtext = c("~N"))

ATV <- data$ccbd_mean_length
datos.l<-fitdist(ATV, "norm")
cdfcomp(list(datos.l), xlogscale = TRUE, 
        ylogscale = TRUE, legendtext = c("~N"))

```

### Test formal de normalidad.

```{r}
library(EnvStats)
library(nortest)
shapiro.test(datasinout$ccbd_mean_length)
ad.test(datasinout$ccbd_mean_length) #prueba de normalidad de Anderson-Darling

shapiro.test(data$ccbd_mean_length)
ad.test(data$ccbd_mean_length) #prueba de normalidad de Anderson-Darling

```

### Analítica NO PARAMÉTRICA; libre distribución

### Estudio de

```{r}
bw.nrd0(datasinout$ccbd_mean_length) #regla general de Silverman
bw.nrd(datasinout$ccbd_mean_length)  #regla general de Scott
bw.bcv(datasinout$ccbd_mean_length)  #convalidación cruzada
bw.ucv(datasinout$ccbd_mean_length)  #validación cruzada imparcial
bw.SJ(datasinout$ccbd_mean_length)   #Sheather & método Jones

density.default(x = datasinout$ccbd_mean_length, kernel = "gaussian")
density.default(x = datasinout$ccbd_mean_length, kernel = "epanechnikov")
density.default(x = datasinout$ccbd_mean_length, bw="nrd", kernel = "gaussian")
density.default(x = datasinout$ccbd_mean_length, bw="SJ", kernel = "gaussian")
density.default(x = datasinout$ccbd_mean_length, bw="bcv", kernel = "gaussian")
```

```{r}
require(graphics)

plot(density(datasinout$ccbd_mean_length))  # IQR = 0

# The Old Faithful geyser data
d <- density(datasinout$ccbd_mean_length, bw = "sj")
d
plot(d)

## Missing values:
x <- xx <- datasinout$ccbd_mean_length
x[i.out <- sample(length(x), 4)] <- NA
doR <- density(x, bw = 2.929, na.rm = TRUE)
lines(doR, col = "blue")
points(xx[i.out], rep(0.01, 4))

## Weighted observations:
fe <- sort(datasinout$ccbd_mean_length) # has quite a few non-unique values
## use 'counts / n' as weights:
dw <- density(unique(fe), weights = table(fe)/length(fe), bw = d$bw)
utils::str(dw) ## smaller n: only 126, but identical estimate:
stopifnot(all.equal(d[1:3], dw[1:3]))

## simulation from a density() fit:
# a kernel density fit is an equally-weighted mixture.
fit <- density(xx)
N <- length(datasinout$ccbd_mean_length)
x.new <- rnorm(N, sample(xx, size = N, replace = TRUE), fit$bw)
plot(fit)
lines(density(x.new), col = "blue")


(kernels <- eval(formals(density.default)$kernel))

##-------- Semi-advanced theoretic from here on -------------
## <!-- %% i.e. "secondary example" in a new help system ... -->
(RKs <- cbind(sapply(kernels,
                     function(k) density(kernel = k, give.Rkern = TRUE))))
100*round(RKs["epanechnikov",]/RKs, 4) ## Efficiencies
bw <- bw.SJ(datasinout$ccbd_mean_length) ## sensible automatic choice
plot(density(datasinout$ccbd_mean_length, bw = bw),
     main = "same sd bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(datasinout$ccbd_mean_length, bw = bw, kernel = kernels[i]), col = i)

## Bandwidth Adjustment for "Exactly Equivalent Kernels"
h.f <- sapply(kernels, function(k)density(kernel = k, give.Rkern = TRUE))
(h.f <- (h.f["gaussian"] / h.f)^ .2)
## -> 1, 1.01, .995, 1.007,... close to 1 => adjustment barely visible..

plot(density(datasinout$ccbd_mean_length, bw = bw),
     main = "equivalent bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(datasinout$ccbd_mean_length, bw = bw, adjust = h.f[i], kernel = kernels[i]),
         col = i)
legend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)

```

### 
