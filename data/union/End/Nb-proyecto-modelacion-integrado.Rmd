---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(MASS)
library(fitdistrplus)
library(ggstatsplot)
library(tibble)
library(readr)
```

# Proyecto de modelación integrado en etapas dentro de la notebook

## Etapa 1 - definición de los datos expuestos y no expuestos

Esta parte realiza la lectura de datos y los convierte en trame

```{r}
# confusoras DAG= "sl2l_qa", "sl2l_iso","ecpl_qa", "ecpl_iso","age", "gender_F", 
# features predictoras = "sl2l_diameter","ccbd_diameter","ifol_diameter","ecpl_volume","tral_volume","sl2l_mean_length","ccbd_mean_length"

setwd("C:/hcgalvan/Repositorios/hcgalvan_project/data/union/End")
temp = gsub(".*target.*", "", readLines("integrado.csv"))
data<-read.table(text=temp, sep=",", header=TRUE)

t<-data.frame(data[,c("sl2l_diameter","ccbd_diameter","ifol_diameter","ecpl_volume","tral_volume","sl2l_mean_length","ccbd_mean_length","label","sl2l_qa", "sl2l_iso","ecpl_qa", "ecpl_iso","age", "gender_F")])

t$gend_F <- ifelse(t$gender_F == "True", 1, 0) #convierto en = o 1
t <- subset(t, select = -c(gender_F)) # Quito de tabla 
```

### Creación de features expuestos y no expuestos

Definición de nuevas variables como Expuesto y no Expuesto Se realizó estudio de cada variable: Su real distribución empírica Esta distribución se obtiene de los sujetos de control. las constantes definidas son el 1ero y 3er cuartil obtenida de distribución de densidad

False = No Expuesto = 0; True = Expuesto = 1

```{r}

t<- t %>% 
  mutate(sl2ldmExp = if_else((t$sl2l_diameter > 22.10 & t$sl2l_diameter < 24.42), 0, 1))

t<- t %>% 
  mutate(ccbdmExp = if_else((t$ccbd_diameter > 14.68 & t$ccbd_diameter < 16.51), 0, 1))

t<- t %>% 
  mutate(ifoldmExp = if_else((t$ifol_diameter > 14.68 & t$ifol_diameter < 16.51), 0, 1))

t<- t %>% 
  mutate(ecplvlExp = if_else((t$ecpl_volume > 6455 & t$ecpl_volume < 8918), 0, 1))

t<- t %>% 
  mutate(tralvlExp = if_else((t$tral_volume > 12366 & t$tral_volume < 18004), 0, 1))

t<- t %>% 
  mutate(sl2lmlExp = if_else((t$sl2l_mean_length > 72.18 & t$sl2l_mean_length < 80.61), 0, 1))

t<- t %>% 
  mutate(ccbdmlExp = if_else((t$ccbd_mean_length > 112.62 & t$ccbd_mean_length < 123.81), 0, 1))
```

### Guardar datos para su estudio en InfoStat 

El estudio que realizo en InfoStat es para determinar que variable predictora debe estar en la regresión y continuar el estudio por R. InfoStat me arroja rapidamente cuales son las confusoras más importantes

Dependerá de variable A y las confusoras.

```{r}
write.csv(t, "C:\\hcgalvan\\Repositorios\\hcgalvan_project\\data\\union\\End\\dbfmodelo.csv", row.names=FALSE)
```

### Creación de variable A a partir de los expuestos y no expuestos

OPCION 1: Variable con A con 5 features filtrados como expuestos.

| t\<- t %\>% mutate(A = if_else((t$sl2ldmExp==1 & t$sl2lmlExp==1 & t$ccbdmExp==1 & t$ccbdmlExp==1 & t\$tralvlExp==1),1, 0)) # No agrego ecpl porque contiene mucho ruido

OPCION 2: Variable como A con 5 features filtrados como expuestos

| t\<- t %\>% mutate(A = if_else((t$sl2ldmExp==1 & t$sl2lmlExp==1 & t$ccbdmExp==1 & t$ccbdmlExp==1 & t$tralvlExp==1 & t$ifoldmExp ==1),1, 0)) # No agrego ecpl porque contiene mucho ruido filter(t, A==1)

OPCION 3: Variable A Con una sola variable filtrada

```{r}
t<- t %>% 
  mutate(A = if_else(t$sl2ldmExp==1,1, 0))

```

## Etapa 2: Regresion Logística con PCA

Por lo que está sucediendo con los datos cuando trabajamos por features como A, y no en su conjunta, es que vamos a necesitar utilizar PCA para que pueda tener predictoras más ajustadas, que solo las elegidas. Esto se puede observar en la regresión logística obtenida, ya que no supera los p-value correctos. Es decir, no converge.

```{r}
t2 <- dplyr::select(dplyr::filter(data, label == 0), "sl2l_diameter","sl2l_qa", "sl2l_iso", "ccbd_iso", "ccbd_qa", "ifol_qa", "ifol_iso", "ecpl_qa", "ecpl_iso", "tral_qa", "tral_iso", "afsl_qa", "afsl_iso", "afsr_qa", "afsr_iso", "cfpl_iso", "cfpl_qa", "cfpr_iso", "cfpr_qa", "fatl_iso", "fatl_qa", "fatr_iso", "fatr_qa", "slfl_iso", "slfl_qa", "slfr_iso", "slfr_qa", "tral_iso", "tral_qa", "ufsl_iso", "ufsl_qa", "ufsr_iso", "ufsr_qa", "age", "gender_F")

## Creo variable gender en una sola variable
t2$gend_F <- ifelse(t2$gender_F == "True", 1, 0) #convierto en = o 1
t2 <- subset(t2, select = -c(gender_F)) # Quito de tabla 

##Creo la variable A desde sl2l_diameter
t2<- t2 %>% 
  mutate(A = if_else((t2$sl2l_diameter > 22.10 & t2$sl2l_diameter < 24.42), 0, 1))
t2 <- subset(t2, select = -c(sl2l_diameter)) # Quito de tabla 

t2
```

### FORMA 1

```{r}
library(caret) #datapartition
library(mlbench)
inTrain <- createDataPartition(y=t2$A,
                               p = 0.70,
                               list = FALSE)

training <- t2[inTrain, ]
testing <- t2[-inTrain, ]
training[,c(-33)]
# Using PCA preprocessing data --------------------------------------------
pca.out <- prcomp(training[,c(-33)],
                  
                  scale. = TRUE)
pca.out
#pca.out
biplot(pca.out, scale = 0)
pca.var <- pca.out$sdev^2

pve <- pca.var/sum(pca.var)
plot(pve, xlab = "Principal component", 
     ylab = "Proportion of variation explained",
     ylim = c(0, 1), 
     type = 'b')

plot(cumsum(pve), xlab = "Principal component", 
     ylab = "Accumulative Prop. of variation explained",
     ylim = c(0, 1), 
     type = 'b')
library(factoextra)
fviz_screeplot(pca.out, addlabels = TRUE)
# Cálculo de la varianza explicada acumulada 
prop_varianza <- pca.out$sdev^2/sum(pca.out$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)
ggplot(data = data.frame(prop_varianza_acum, pc = factor(1:32)),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum,2))) +
  theme_bw() +
  labs(x = "Componentes principales", 
       y = "Prop. varianza explicada acumulada")
################################
# Construct new data frame with PCA selected compoents 1:10 ---------------
#training
midatos <- data.frame(A = training[, "A"], pca.out$x[, 1:2])

# Fit the logistic regressin model again ----------------------------------

logRegPCA <- glm(A ~ .,  data = midatos, family = binomial)

# Predict the scores on PC1 for the test set data
# head(testing)
test.p <- predict(pca.out, newdata = testing[, 1:33])
#head(test.p)

# Now use that to predict the class
pred <- predict(logRegPCA, newdata = data.frame(test.p[, 1:5]), type = "response")
summary(pred)
#test.p[, 1:5]
# cross-classification table 
predA <- factor(ifelse(pred >= 0.5, 1, 0))
#
table(predA)
table(testing$A)

table(testing$A, predA)
testing$A
as.factor(predA)
confusionMatrix(testing$A, as.factor(predA))

# Sensibilidad= 1 significa que tuvo algunos verdaderos positivos y ningún falso negativo: todos los casos reales se predijeron correctamente como positivos

# Especificidad= 0 significa que tuvo algunos falsos positivos y ningún verdadero negativo: todos los no casos reales se predijeron incorrectamente como positivos

# Entonces, tener ambos significa que se predijo que todo sería positivo, ya fuera un caso real o no.

```

### FORMA 2

```{r}
pca.out2 <- prcomp(t2[,c(-33)],
                  scale. = TRUE)
pca.out2
misdatos <- data.frame(A = t2[, "A"], pca.out2$x[, 1:2])
pca.out2[1]
logRegPCA <- glm(A ~ .,  data = misdatos, family = binomial)
summary(logRegPCA)
logistic.regression.or.ci(logRegPCA)
summary(logRegPCA)$coefficients

```

## Etapa 2: Regresión Logística entre A y Confusoras

Aquí es un trabajo manual para cada prueba a realizar. Cada regresión tiene que ser ajustada.

```{r}

tag<-filter(t, label==0)
### MODELO 1
# Modelo de regresión logística
form = "A ~ age + gend_F + sl2l_iso + ecpl_iso"
form = formula(form)
model1 <- glm(form, data = tag, family = binomial())
summary(model1)
step(model1, direction = "backward")

### MODELO 2
form = "A ~ age + gend_F+ sl2l_iso"
form = formula(form)
model2 <- glm(form, data = tag, family = binomial())
summary(model2)
step(model2, direction = "backward")

logistic.regression.or.ci(model2)

# Bondad de ajuste del modelo Devianza y Chi2
# Para saber la eficacia del modelo prediciendo la variable respuesta utilizamos el estadístico chi-cuadrado, que mide la diferencia entre el modelo en su estado actual y el modelo cuando sólo se incluyó la constante.

dev <- model2$deviance
nullDev <- model2$null.deviance
modelChi <- nullDev - dev
modelChi


#como la probabilidad es menor que 0.05, podemos rechazar la hipótesis nula de que el modelo es mejor prediciendo la variable resultado que si elegimos por azar. Por tanto, podemos decir que, en general, el modelo tiene una aportación significativa en la perdición
chigl <- model2$df.null - model2$df.residual
chisq.prob <- 1 - pchisq(modelChi, chigl)
chisq.prob

# R^2
R2.hl <- modelChi/model2$null.deviance
R2.hl
#coeficientes y z-statistic
summary(model2)$coefficients

# Odds ratio
exp(model2$coefficients)
## intervalos de confianza
exp(confint(model2))


# Diagnóstico del modelo
tag$probabilidades.predichas <- fitted(model2)
tag$studentized.residuals <- rstudent(model2)
tag$dfbeta <- dfbeta(model2)
tag$dffit <- dffits(model2)
tag$leverage <- hatvalues(model2)

head(tag[, c("age", "sl2l_iso","probabilidades.predichas")])
head(tag[, c("leverage", "studentized.residuals", "dfbeta")])

# Selección del modelo
modelog <- glm(A ~ age+sl2l_iso, data = tag, family = binomial())
summary(modelog)
step(modelog, direction = "backward")

####################################################
# Supuestos del modelo Linealidad
tag$logageInt <- log(tag$age) * tag$age
tag$logsl2lInt <- log(tag$sl2l_iso) * tag$sl2l_iso
#df$logvar2Int <- log(df$var2) * df$var2


```

### Generación de Z Score

Usar una de ellas. La que mejor se ajusta a los datos.

### Con PCA

```{r}
pca.out2[1]

exp_z <- 1/(1+exp(-(as.numeric(unlist(logRegPCA$coefficients[1]))+as.numeric(unlist(logRegPCA$coefficients[2]))*as.numeric(unlist(pca.out2[1]))+as.numeric(unlist(logRegPCA$coefficients[3]))*as.numeric(unlist(pca.out2[2])))))

plot(exp_z)

```

### SIN PCA

```{r}
### Utilizo el modelo 2
t<- t %>% 
        mutate(zscore = (1/(1+exp(-(as.numeric(unlist(model2$coefficients[1]))+as.numeric(unlist(modelog$coefficients[2]))*data$age+as.numeric(unlist(model2$coefficients[3]))*data$sl2l_iso)))))

```

#### Armado de Subclases del Zscore para estudio posterior

Tener en cuenta que se puede dividir de manera diferente, por otra cantidad de estratos como por ejemplo spline.

```{r}
#########
# Entonces utilizo el total
t$subclass <- cut(x=t$zscore,
                              breaks=quantile(t$zscore, 
                              prob = seq(0, 1, 1/5)),include.lowest=T)
levels(t$subclass) <- 1:length(levels(t$subclass))
#examine common support
xtabs(~A+subclass,t) #table of counts per stratum
```

#### Generar Función constante por partes/escalonada Lambda

Aquí es una función que se obtiene de splines. No la tenemos todavía.

#### Generar Constante Delta

Se utiliza o genera despues de realizar la regresión

```{r}
#Regresion logística condicional o por estratos
reglog.cond <-clogit(label~A+strata(subclass),data=t)
summary(reglog.cond)
delta = coefficients(reglog.cond)[[1]]
```

## Modelo Final

El modelo final tiene algunas dificultades, con respecto a si es por cada sujeto o es el total de sujetos.

Deberíamos tener dos calculos

```{r}
# Función  constante por partes o escalonada
# La idea que entiuendo es: detras es que los valores de puntación de zscore está a partir de los sujetos de estudio, y se arma nudos para capturar las medias o puntos centrales.

# Definida en base a splines // buscando la función
# x<-zsc

#cuts <- c( -Inf, -1.793, -1.304, -0.326, 0.625, 1.630, 2.119 )
#vals <- c(    0,  0.454,      0,  0.632,     0, 0.227,     0 )
#fx <- vals[findInterval(x, c(-Inf, cuts))]
#f <- stepfun(cuts[-1], vals)
#fx <- f(x)
##################################################################
#función productoria
productoria <- function(D,delta,A,lamdaz){
   result = prod((exp(D*(delta*A)+lamdaz))/(1+exp((delta*A)+lamdaz)))
   return(result)
}
productoria(1,1,1,1)
## donde D es label donde es 1 = Patologia, 0 sin patología, 
## delta es la varianza de regresion logística estratificada entre A y estratos de los cuantiles de la puntuación de zscore 
## 

v<-c()

# delta = -0.6558704 este valor de delta proviene de reg log estrat

for(i in 1:nrow(t)) {       # for-loop over rows
  v[i] <- (productoria(t[i,c("label")], delta, t[i,c("A")], t[i,c("zscore")]))
}

plot(v)
```
