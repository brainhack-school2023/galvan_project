---
title: "Analitica tral_volume"
output: html_notebook
---

### Conclusiones del Análisis tral_volume

El feature ecpl_volume tiene básicos trabajos de limpieza para lograr un estimador de posición y establecer su distribución.

Estos pasos siguientes fueron realizados a partir de exploración gráfica de los datos y bajo conocimiento de su captura y procesamiento.

**Paso 1**: separar valores en control y estudio.\
**Paso 2:** verificar valores repetidos.En este caso no existen repetidos.\
**Paso 3:** Limpiar los outliers, si bien separados en Control y Estudio no tienen, si existe cuando se juntan ambas particiones.

Luego, verificando y ajustando a distribuciones teóricas, se asemeja a la distribución normal. Realizamos los ajustes, aunque sus datos no logran ajustar del todo.\
También, realizamos pruebas con distribución libre o sin Ajuste total a una distribución teórica, utilizamos diferentes nucleos y ancho de banda.

Con prechequeo formal formal de normalidad (**shapiro-wilk y Anderson-Darling)**, sostiene la hipótesis de normalidad, aunque no ajuste correctamente a los datos, se indaga además en la estimación de densidad con núcleo y ancho de banda para lograr mayor aproximación a la distribución de densidad de la muestra, con estimadores más próximos.

Los selectores de ancho de banda como: **Silverman** o **Scott son eficientes y rápidos con distribución normal**. También es viable utilizar la versión que más ajustada cuando tenemos dudas de distribución: **método Sheather y Jones**.

Básicamente, estamos indicando que los estimadores seguirán el nucleo y ancho de banda utilizado, aportando mayor flexibilidad, aunque perdiendo eficiencia.

La decisión está en sus estimadores y ajuste de distribución. Cuando ajustamos a una distribución teórica de la normal (es asintóticamente normal y de consistencia fuerte, es decir tanto la media y sd son insesgado) , observamos que sus estimadores de muestra limpia sin duplicados son iguales a la **media = 14703.747 y sd = 4578.808**, con error Std mayor que la obtenida en datos puros de la base, sin embargo estos datos de base tienen duplicados por ajustes y no por composición natural, e invalida cualquier conclusión comparado relacionado al error estandar entre estos estimadores. Observando de manera general, no logra ajustar correctamente la muestra a distribución normal.

```         
Ajuste Parcial de Outliers
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.     
3741   12059   15040   14830   17741   26424     

Ajuste Total de outliers en 2 fases
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.     
3741   12020   14992   14704   17724   25258     

Sin Ajustes en muestra observada
Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    
318.3 11816.0 14886.0 14393.8 17724.4 26423.8 
```

Ahora bien, utilizando la muestra limpia de duplicados y outliers, al tomar una distribución libre (de comportamiento asintótico debilmente consistentes, su media es insesgado y sd posee sesgo), media y mediana se igualan **= 14499.2** , y no posee restricciones de supuestos; aunque sus cuartiles varían en base al método seleccionado. Nuevamente recordar que si nuestra distribución se acerca a una normal, el método Silverman, es sólido, donde ***1Q=*** **6793.8** ***y 3Q=*****22204.6***. También es importante destacar los estimadores utilizados en Silverman y Scott poseen propiedades en este método que permiten utilizar LGN y TCL. Por otro lado podemos comparar con el método más libre* **Sheather y Jones *1Q=*6715** ***y 3Q=*****22283*, si bien este modelo no posee todas las propiedades de Silverman y Scott.***

> **Podemos concluir, que la versión de Silverman se ajusta apropiadamente a la distribución de estos datos del feature tral_volume**

#### Con Ajuste a una distribución teórica

| Normal ajustado completamente
|             estimate Std. Error  mean defined   sd
| 
| Normal ajustado parcialmente
| 
| Normal sin ajustes
|              estimate Std. Error  mean   sd
| 

| Call:   density.default(x = datasinout1\$tral_volume, kernel = "gaussian")
| Data: datasinout1\$tral_volume (92 obs.);     Bandwidth 'bw' = 1551
| **Utilizo: Metodo Silverman**

|        x                 y           
|  Min.   : -911.6   Min.   :3.310e-08 
|  1st Qu.: 6793.8   1st Qu.:5.937e-06 
|  **Median :14499.2** Median :2.451e-05 
|  **Mean   :14499.2** Mean   :3.241e-05 
|  3rd Qu.:22204.6   3rd Qu.:5.320e-05 
|  Max.   :29910.0   Max.   :9.590e-05 
| 
| Call:   density.default(x = dataendcleanun, **bw = "nrd0"**,kernel = "epanechnikov")
| Data: dataendcleanun (73 obs.);       Bandwidth 'bw' = 1199
| **Utilizo: Metodo Silverman (Es solida si son cercanas a distribución normal, sin embargo en no funciona bien en casos complicados.)**
|        x                 y           
|  Min.   : -911.6   Min.   :3.310e-08 
|  1st Qu.: 6793.8   1st Qu.:5.937e-06 
|  **Median :14499.2** Median :2.451e-05 
|  **Mean   :14499.2** Mean   :3.241e-05 
|  3rd Qu.:22204.6   3rd Qu.:5.320e-05 
|  Max.   :29910.0   Max.   :9.590e-05 
| 
| Call:   density.default(x = datasinout1\$tral_volume, **bw = "nrd"**, kernel = "gaussian")
| Data: datasinout1\$tral_volume (92 obs.);     Bandwidth 'bw' = 1551
| **Utiliza Método Scott (Requerido tener distribución normal para tener significancia)**
|        x                 y           
|  Min.   : -911.6   Min.   :0.000e+00 
|  1st Qu.: 6793.8   1st Qu.:6.471e-06 
|  **Median :14499.2** Median :2.368e-05 
|  **Mean   :14499.2** Mean   :3.241e-05 
|  3rd Qu.:22204.6   3rd Qu.:5.110e-05 
|  Max.   :29910.0   Max.   :9.229e-05 
| 
| Call:   density.default(x = datasinout1\$tral_volume, **bw = "SJ"**, kernel = "gaussian")
| Data: datasinout1\$tral_volume (92 obs.); Bandwidth 'bw' = 1603
| Metodo **Sheather y Jones (Solo considera una distribución suave)**
|        x               y           
|  Min.   :-1069   Min.   :3.230e-08 
|  1st Qu.: 6715   1st Qu.:5.576e-06 
|  **Median :14499** Median :2.417e-05 
|  **Mean   :14499** Mean   :3.208e-05 
|  3rd Qu.:22283   3rd Qu.:5.268e-05 
|  Max.   :30067   Max.   :9.497e-05 
| 
| Call:   density.default(x = datasinout1\$tral_volume, **bw = "bcv"**, kernel = "gaussian")
| Data: datasinout1\$tral_volume (92 obs.); Bandwidth 'bw' = 2121
| Método Cross Validation Completo, utiliza de fondo selector de ancho de banda no paramétrico. Lo que implica solo tener
|        x               y           
|  Min.   :-2622   Min.   :2.733e-08 
|  1st Qu.: 5939   1st Qu.:3.296e-06 
|  Median :14499   Median :2.054e-05 
|  Mean   :14499   Mean   :2.917e-05 
|  3rd Qu.:23060   3rd Qu.:4.926e-05 
|    Max.   :31620   Max.   :8.733e-05 

```{r}
library(ggplot2)
library(gridExtra)
library(MASS)
library(fitdistrplus)
library(ggstatsplot)
library(BSSasymp) #Cramer-ca

```

```{r}
### Ubicacion de datos
setwd("C:/hcgalvan/Repositorios/hcgalvan_project/data/union/End")
temp = gsub(".*target.*", "", readLines("seleccionestudio.csv"))
data<-read.table(text=temp, sep=",", header=TRUE)
tralvl <-data$tral_volume

```

#### Tratamiento Ajustado a Distribución y comparativa con/sin outliers

Separamos los datos de la variable de estudio

```{r}
Control<-(as.numeric(unlist(subset(data, label==1, select=c(tral_volume)))))
Estudio<-(as.numeric(unlist(subset(data, label==0, select=c(tral_volume)))))
Controlclean<-unique(Control)
Estudioclean<-unique(Estudio)

```

```{r}
boxplot(Control, Estudio, tralvl)$out
```

```{r}
Q <- quantile(data$tral_volume, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(data$tral_volume)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
datasinout<- subset(data, data$tral_volume > (Q[1] - 1.5*iqr) & data$tral_volume < (Q[2]+1.5*iqr))
```

```{r}
Q <- quantile(datasinout$tral_volume, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(datasinout$tral_volume)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
datasinout1<- subset(datasinout, datasinout$tral_volume > (Q[1] - 1.5*iqr) & datasinout$tral_volume < (Q[2]+1.5*iqr))
```

```{r}
boxplot(datasinout$tral_volume, datasinout1$tral_volume)$out
```

Graficamos las distribuciones obtenidos.

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) 
plotdist(datasinout1$tral_volume, histo=TRUE, demp=TRUE) #completamente ajustado
plotdist(datasinout$tral_volume, histo=TRUE, demp=TRUE) #completamente ajustado
plotdist(data$tral_volume, histo=TRUE, demp=TRUE) #completamente ajustado

```

Realizamos descripcion de los valores descriptivos: Ajuste total, parcial y sin ajuste

```{r}
summary(datasinout$tral_volume) 
summary(datasinout1$tral_volume) #Ajuste completo
summary(data$tral_volume)

```

Graficamos la densidad de distribución: Ajuste Total, Parcial y sin ajustes

```{r}
descdist(datasinout1$tral_volume, boot = 1000) 
descdist(datasinout$tral_volume, boot = 1000)
descdist(data$tral_volume, boot = 1000)
```

Ajustamos los datos a la distribución Normal

```{r}
library(glogis) 
total<-fitdist(datasinout1$tral_volume, "norm")
total2<-fitdist(datasinout$tral_volume,"norm")
sinajuste<-fitdist(data$tral_volume, "norm") 

summary(total) 
summary(total2)
summary(sinajuste) 

```

```{r}

```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) 
fb <- fitdist(datasinout1$tral_volume, "norm") 
plot.legend <- c("norm") 
denscomp(list(fb), legendtext = plot.legend) 
qqcomp(list(fb), legendtext = plot.legend) 
cdfcomp(list(fb), legendtext = plot.legend) 
ppcomp(list(fb), legendtext = plot.legend)
```

```{r}
library(actuar)  

ATV2 <- datasinout1$tral_volume
datos2.l<-fitdist(ATV2, "norm")
cdfcomp(list(datos2.l), xlogscale = TRUE, 
        ylogscale = TRUE, legendtext = c("~N"))

ATV1 <- datasinout$tral_volume
datos1.l<-fitdist(ATV1, "norm")
cdfcomp(list(datos1.l), xlogscale = TRUE, 
        ylogscale = TRUE, legendtext = c("~N"))

ATV <- data$tral_volume
datos.l<-fitdist(ATV, "norm")
cdfcomp(list(datos.l), xlogscale = TRUE, 
        ylogscale = TRUE, legendtext = c("~N"))

```

```{r}
quantile(datos.l, probs = 0.05) # Sin Ajustes de outliers ~Normal
quantile(datos1.l, probs = 0.05) # Sin Ajustes de outliers ~Normal
quantile(datos2.l, probs = 0.05) # Parcialmente Ajustados de outliers ~Normal
```

### Test formal de normalidad.

```{r}
library(EnvStats)
library(nortest)
shapiro.test(datasinout1$tral_volume)
ad.test(datasinout1$tral_volume) #prueba de normalidad de Anderson-Darling

shapiro.test(datasinout$tral_volume)
ad.test(datasinout$tral_volume) #prueba de normalidad de Anderson-Darling

shapiro.test(data$tral_volume)
ad.test(data$tral_volume) #prueba de normalidad de Anderson-Darling

```

### Analítica NO PARAMÉTRICA; libre distribución

### Estudio de

```{r}
bw.nrd0(datasinout1$tral_volume) #regla general de Silverman
bw.nrd(datasinout1$tral_volume)  #regla general de Scott
bw.bcv(datasinout1$tral_volume)  #convalidación cruzada
bw.ucv(datasinout1$tral_volume)  #validación cruzada imparcial
bw.SJ(datasinout1$tral_volume)   #Sheather & método Jones

density.default(x = datasinout1$tral_volume, kernel = "gaussian")
density.default(x = datasinout1$tral_volume, kernel = "epanechnikov")
density.default(x = datasinout1$tral_volume, bw="nrd", kernel = "gaussian")
density.default(x = datasinout1$tral_volume, bw="SJ", kernel = "gaussian")
density.default(x = datasinout1$tral_volume, bw="bcv", kernel = "gaussian")
```

```{r}
require(graphics)

plot(density(datasinout1$tral_volume))  # IQR = 0

# The Old Faithful geyser data
d <- density(datasinout1$tral_volume, bw = "sj")
d
plot(d)

## Missing values:
x <- xx <- datasinout1$tral_volume
x[i.out <- sample(length(x), 10)] <- NA
doR <- density(x, bw = 1550.821, na.rm = TRUE)
lines(doR, col = "blue")
points(xx[i.out], rep(0.01, 10))

## Weighted observations:
fe <- sort(datasinout1$tral_volume) # has quite a few non-unique values
## use 'counts / n' as weights:
dw <- density(unique(fe), weights = table(fe)/length(fe), bw = d$bw)
utils::str(dw) ## smaller n: only 126, but identical estimate:
stopifnot(all.equal(d[1:3], dw[1:3]))

## simulation from a density() fit:
# a kernel density fit is an equally-weighted mixture.
fit <- density(xx)
N <- length(datasinout1$tral_volume)
x.new <- rnorm(N, sample(xx, size = N, replace = TRUE), fit$bw)
plot(fit)
lines(density(x.new), col = "blue")


(kernels <- eval(formals(density.default)$kernel))

##-------- Semi-advanced theoretic from here on -------------
## <!-- %% i.e. "secondary example" in a new help system ... -->
(RKs <- cbind(sapply(kernels,
                     function(k) density(kernel = k, give.Rkern = TRUE))))
100*round(RKs["epanechnikov",]/RKs, 4) ## Efficiencies
bw <- bw.SJ(datasinout1$tral_volume) ## sensible automatic choice
plot(density(datasinout1$tral_volume, bw = bw),
     main = "same sd bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(datasinout1$tral_volume, bw = bw, kernel = kernels[i]), col = i)

## Bandwidth Adjustment for "Exactly Equivalent Kernels"
h.f <- sapply(kernels, function(k)density(kernel = k, give.Rkern = TRUE))
(h.f <- (h.f["gaussian"] / h.f)^ .2)
## -> 1, 1.01, .995, 1.007,... close to 1 => adjustment barely visible..

plot(density(datasinout1$tral_volume, bw = bw),
     main = "equivalent bandwidths, 7 different kernels")
for(i in 2:length(kernels))
   lines(density(datasinout1$tral_volume, bw = bw, adjust = h.f[i], kernel = kernels[i]),
         col = i)
legend(55, 0.035, legend = kernels, col = seq(kernels), lty = 1)

```

### 
